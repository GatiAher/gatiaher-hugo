<!DOCTYPE html>
<html lang="en">
  <head>
  
   
  <title>IMU Gesture Recognition</title>
  
  <meta charset="utf-8" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1.0, user-scalable=no"
  />
  
  
  <link rel="stylesheet" href="/css/main.css" />
    
  <noscript><link rel="stylesheet" href="/css/noscript.css" /></noscript>
  
  

<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <link
    rel="apple-touch-icon"
    sizes="180x180"
    href="/favicon_io/apple-touch-icon.png"
  />
  <link
    rel="icon"
    type="image/png"
    sizes="32x32"
    href="/favicon_io/favicon-32x32.png"
  />
  <link
    rel="icon"
    type="image/png"
    sizes="16x16"
    href="/favicon_io/favicon-16x16.png"
  />
  <link rel="manifest" href="/favicon_io/site.webmanifest" />
  
  <meta name="generator" content="Hugo 0.108.0"> <meta property="og:title" content="IMU Gesture Recognition" />
<meta property="og:description" content="This project analyzed and prepared a baseline machine learning model to perform gesture recognition on data collected with MbientLab MMRL IMU rubber-banded to a two_finger ring. The final baseline model was trained with data from sessions 4, 5, and 7 consisting of 1,212 total instances of 4 gestures collected across 27 people.
When trained with a train-test split of 80:20, the model had an accuracy of 75%. The final model trained with full data (no train-test split) had reasonably robust performance in the real-time system (successfully generalized its gestures predictions to other people when integrated with the software demo app).
This report provides details on deciding on a gesture set, building and refining the gesture data collection process, and steps to integrate the model with the software iOS demo app.

  
       
    
      Visit GitHub
    
    
  





 
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://GatiAher.github.io/projects/imu-gesture-recognition/" /><meta property="og:image" content="http://GatiAher.github.io/img/cover_page.png"/><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2022-05-15T14:53:05-04:00" />
<meta property="article:modified_time" content="2022-05-15T14:53:05-04:00" />

</head>


  <body class="is-preload">
    
    
<div>
  <header id="header">
  <a href="/" class="title">Gati Aher</a>
  <nav>
    <ul>
      
      
      <li>
        <a href="http://GatiAher.github.io/categories/software-development/">Software Development (8)</a>
      </li>
      
      <li>
        <a href="http://GatiAher.github.io/categories/concepts-theory/">Concepts &amp; Theory (6)</a>
      </li>
      
      <li>
        <a href="http://GatiAher.github.io/categories/data-analysis/">Data Analysis (5)</a>
      </li>
      
      <li>
        <a href="/artwork">Art</a>
      </li>
    </ul>
  </nav>
</header>


  <div id="wrapper">
    
    <section id="single" class="wrapper">
      <header>
        <div class="single-header">
             
<a class="category" href="/categories/data-analysis">Data Analysis</a>
  

          <h1>IMU Gesture Recognition</h1>
          <p>
            <i>
              written by Gati Aher on 

<time class="date" datetime="2022-05-15">May 15, 2022</time> | 15 min read <br />
                  tags:
<a href="/tags/olin-college-sp2022-social-technology-enterprise-with-purpose-step">Olin College: SP2022 Social Technology Enterprise with Purpose (STEP)</a>
  

            </i>
          </p>
        </div>
      </header>
      <div class="inner">
        <article><p>This project analyzed and prepared a baseline machine learning model to perform gesture recognition on data collected with <a href="https://mbientlab.com/store/metamotionrl/">MbientLab MMRL IMU</a> rubber-banded to a two_finger ring. The final baseline model was trained with data from sessions 4, 5, and 7 consisting of 1,212 total instances of 4 gestures collected across 27 people.</p>
<p>When trained with a train-test split of 80:20, the model had an accuracy of 75%. The final model trained with full data (no train-test split) had reasonably robust performance in the real-time system (successfully generalized its gestures predictions to other people when integrated with the software demo app).</p>
<p>This report provides details on deciding on a gesture set, building and refining the gesture data collection process, and steps to integrate the model with the software iOS demo app.</p>
<div class="center">
  <ul class="actions">
       
    <li>
      <a href="https://github.com/OlinSTEP/signal-processing-gesture-data-collection" class="button icon outline brands fa-github">Visit GitHub</a>
    </li>
    
  </ul>
</div>



<div id="Container"
 style="padding-bottom:56.25%; position:relative; display:block; width: 100%">
 <iframe id="googleSlideIframe"
  width="100%" height="100%"
  src="https://docs.google.com/presentation/d/e/2PACX-1vT1MJqD9l2cEu7DgdtxDkJb_aH5ysP1NklfoNUuvb3sVjp9z3MezS0HFBOv-fLhV7ESwZks_xU1Z-wQ/embed?start=false&amp;loop=true&amp;delayms=3000"
  frameborder="0" allowfullscreen=""
  style="position:absolute; top:0; left: 0"></iframe>
</div>
<div class="toc">
    <p>
        <strong>Table of Contents</strong>
    </p>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#baseline-model">Baseline Model</a>
      <ul>
        <li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
        <li><a href="#support-vector-classifier-svc">Support Vector Classifier (SVC)</a></li>
      </ul>
    </li>
    <li><a href="#gesture-set-creation-process">Gesture Set Creation Process</a>
      <ul>
        <li><a href="#meta-data-spreadsheets">Meta-Data Spreadsheets:</a></li>
        <li><a href="#session-1-and-session-2-march-30-trouble-shooting-process">Session 1 and Session 2 (March 30): Trouble-Shooting Process**</a></li>
        <li><a href="#session-3-march-30-successful-data-collection">Session 3 (March 30): Successful Data Collection</a></li>
        <li><a href="#session-4-april-2-successful-data-collection----1st-big-data-set----1-subject-gati">Session 4 (April 2) Successful Data Collection &ndash; 1st Big Data Set &ndash; 1 Subject (Gati)</a></li>
        <li><a href="#session-5-april-5-successful-data-collection----2nd-big-data-set----16-subjects-step-students">Session 5 (April 5) Successful Data Collection &ndash; 2nd Big Data Set &ndash; 16 subjects (STEP students)</a></li>
        <li><a href="#demo-day-1-april-7">Demo Day 1 (April 7)</a></li>
        <li><a href="#unfortunate-timing-april-10---april-23">Unfortunate Timing (April 10 - April 23)</a></li>
        <li><a href="#session-6-april-25-trouble-shooting-process">Session 6 (April 25) Trouble-Shooting Process</a></li>
        <li><a href="#session-7-april-27-28-successful-data-collection----3rd-big-data-set----16-subjects-olin--prospective-students">Session 7 (April 27-28) Successful Data Collection &ndash; 3rd Big Data Set &ndash; 16 subjects (Olin / Prospective students)</a></li>
        <li><a href="#demo-day-2-may-2-successful-ml-model-integration">Demo Day 2 (May 2) Successful ML-Model Integration</a></li>
      </ul>
    </li>
    <li><a href="#next-steps">Next Steps</a>
      <ul>
        <li><a href="#model">Model</a></li>
        <li><a href="#data-collection">Data Collection</a></li>
        <li><a href="#software-integration">Software Integration</a></li>
      </ul>
    </li>
  </ul>
</nav>
</div>
<h2 id="baseline-model">Baseline Model</h2>
<p>A simple baseline model was used to verify the general robustness of the gesture set and performance of the model. This baseline was comprised of sklearn&rsquo;s standard scalar, Principal Component Analysis (PCA) with the maximum number of components, and a Support Vector Classifier (SVC).</p>
<p>This model was intentionally kept very simple in order to serve as a comparison baseline and use initial prototyping time to build out the data collection and software integration infrastructure.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># very simple baseline model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># x_train and x_test are arrays of sample_number by 300 (100 time-points of each x, y, and z accelerations)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># y_train and y_test are a lists of true gesture labels </span>
</span></span><span style="display:flex;"><span>clf <span style="color:#f92672">=</span> make_pipeline(StandardScaler(), PCA(n_components<span style="color:#f92672">=</span>n_pc), SVC())
</span></span><span style="display:flex;"><span>clf<span style="color:#f92672">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span>pred_test <span style="color:#f92672">=</span> clf<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>acc <span style="color:#f92672">=</span> accuracy_score(y_test, pred_test)
</span></span></code></pre></div><h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3>
<figure><img src="img/pca_session_4_and_5_and_7.png"
         alt="PCA plot: each point is a train-set gesture colored by gesture number as projected onto the first two PCA axes (directions of most and second-most variance in the data set)."/><figcaption>
            <p>PCA plot: each point is a train-set gesture colored by gesture number as projected onto the first two PCA axes (directions of most and second-most variance in the data set).</p>
        </figcaption>
</figure>

<p>Gesture Numbers:</p>
<ul>
<li>1: flick</li>
<li>2: downward tap</li>
<li>3: counter clockwise circle</li>
<li>4: swipe right)</li>
</ul>
<p>PCA, the first part of the baseline model, finds a multi-dimensional feature space defined by directions of most variance within the data. It is commonly used as dimensionality-reduction and signal-cleaning measure before applying machine learning. In this case, each gesture is x, y, and z accelerations taken over 100 time-points, for 300 points in total. Some of these points contain redundant (i.e. correlated) information, so PCA summarizes the redundant information and essentially creates an easier signal to learn. <a href="https://gatiaher.github.io/projects/facial-recognition-using-principal-component-analysis/">(Read more about PCA)</a>.</p>
<h3 id="support-vector-classifier-svc">Support Vector Classifier (SVC)</h3>
<p>SVC, the second part of the baseline model, takes the data as projected onto the PCA feature space and learns a line to best separate the classes. If the PCA separates the points well, the SVC has an easier time.</p>
<figure><img src="img/confusion_matrix_session_4_and_5_and_7.png"
         alt="Confusion Matrix shows model performance on the test-set. The diagonal counts data points that were correctly predicted (i.e. true label == predicted label). This model was 75% accurate."/><figcaption>
            <p>Confusion Matrix shows model performance on the test-set. The diagonal counts data points that were correctly predicted (i.e. true label == predicted label). This model was 75% accurate.</p>
        </figcaption>
</figure>

<p>Confusion (incorrect predictions) occurred most frequently between gesture 1 (flick) and the other gestures, with most confusion happening when the model predicted gesture 1 (flick) when the true label was gesture 2 (downwards tap) (21 instances).</p>
<p>Confusion over the flick gesture was expected, as based on user feedback over the course of data collection sessions 4, 5, and 7, the flick gesture was gradually altered to become more comfortable.</p>
<h2 id="gesture-set-creation-process">Gesture Set Creation Process</h2>
<h3 id="meta-data-spreadsheets">Meta-Data Spreadsheets:</h3>
<ul>
<li>Session 1, 2, 3, 4: <a href="https://docs.google.com/spreadsheets/d/1KgfBDTNwSJSkqkBWl58ZlKFpmN-8T7DxcsZyOiCF7l0/edit#gid=0">Gati - exploratory data collection trials</a></li>
<li>Session 5: <a href="https://docs.google.com/spreadsheets/d/1Vl-UpshacoG6l6ha3ogqCW6fWm9wacj1TBtei_HN2yo/edit#gid=1745147814">Big-2-Finger-Ring-Dataset: Internal Use - Gati &amp; Jerry</a></li>
<li>Session 6, 7: <a href="https://docs.google.com/spreadsheets/d/1WjEH-O8SLWliBAOO7MYLe_ogYH4gCEWHYkiBPFWLnXo/edit#gid=1124057491">Yellow Prompts Data Collection &ndash; STEP &amp; Olin Subjects</a></li>
</ul>
<h3 id="session-1-and-session-2-march-30-trouble-shooting-process">Session 1 and Session 2 (March 30): Trouble-Shooting Process**</h3>
<ul>
<li><em>Gesture Collection App - v1:</em> had to tap on screen to record start and ends of gestures, a separate python script on laptop randomly shuffled gesture order and prompted subject to perform gestures. Prompt &ldquo;make gesture&rdquo; window of 6 seconds</li>
<li><em>Gesture Set - v1:</em> 14 gestures, included swipes, circles, V/checkmark motions, taps</li>
<li><em>Trials:</em>
<ul>
<li>session 1: 5 trials of 14 gestures each, collected on 1 subject (Gati)</li>
<li>session 2: 9 trials of 14 gestures each, collected on 1 subject (Gati)</li>
</ul>
</li>
</ul>
<h4 id="findings--process-iterations">Findings / Process Iterations:</h4>
<ul>
<li>Assume user&rsquo;s hand always returns to neutral after finishing gesture
<ul>
<li>Otherwise hand flies away (ex: do 10 right swipes without returning to neutral/starting position)</li>
<li>Record gesture and return to neutral position as full gesture signal</li>
</ul>
</li>
<li>Gestures must be more creative than just swipe up, swipe down, swipe left, swipe right
<ul>
<li>In swipes, return motion can be confused for a different gesture (ex: do double swipe up, then do double swipe down. Notice that they are the same acceleration patterns)</li>
</ul>
</li>
<li>Gave subject 6 seconds to perform gesture, but that feels very long. Shorten gesture window to collect more gestures and value subjects&rsquo; time</li>
<li>Ask someone (Jerry) to work on redesigning app - it is hard to consistently tap with one hand and gesture with other</li>
</ul>
<h3 id="session-3-march-30-successful-data-collection">Session 3 (March 30): Successful Data Collection</h3>
<ul>
<li><em>Gesture Collection App - v1</em></li>
<li><em>Gesture Set - v1:</em> 14 gestures, give subject 6 seconds to perform a gesture to not risk cutting off a gesture</li>
<li><em>Trials:</em> 9 trials of 14 gestures each, collected on 1 subject (Gati) at 9 different hand orientations (90 deg &ndash; face level to -90 deg &ndash; below chair) to create extra noise</li>
</ul>
<h4 id="findings--process-iterations-1">Findings / Process Iterations:</h4>
<ul>
<li>downloaded json&rsquo;s stored in firebase: tricky because json names are random strings and hard to easily match them to the meta data. Using time-stamps for now, but one accidentally recorded trial could mess everything up?</li>
<li>jupyter notebook <code>session_3/explore_data_session_3.ipynb</code> has code for loading, processing, and analyzing the data</li>
<li>Sample rate is 0.1 ms (100 time points in 1 second)</li>
<li>Window size is typically under 2 seconds long (as recorded from taps to mark beginning and end of gesture)</li>
<li>Note: some taps were awkwardly long because (a) subject forgot to tap, or (b) subject forgot to return hand back to position. Not a perfect intuitive data collection system yet.</li>
<li>It was more common to remember the start tap and forget the end tap</li>
</ul>
<figure><img src="img/tap_diff_determine_window_histogram.png"
         alt="Histogram of seconds (x-axis) between start and end tap vs. occurance count (y-axis)"/><figcaption>
            <p>Histogram of seconds (x-axis) between start and end tap vs. occurance count (y-axis)</p>
        </figcaption>
</figure>

<figure><img src="img/session_3_traces.png"
         alt="Plot of all gesture 1 (swipe left) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)"/><figcaption>
            <p>Plot of all gesture 1 (swipe left) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)</p>
        </figcaption>
</figure>

<h4 id="process-to-evaluate-best-gesture-set">Process to Evaluate Best Gesture Set</h4>
<ul>
<li>Train data: 6 of 9 trials</li>
<li>Test data: 3 of 9 trials (trials 1, 4, and 5 &ndash; randomly chosen (note that test and train data are from different trials, so the test gestures were performed at a different hand orientation from the train gestures. This is a way of checking for gesture robustness)</li>
<li>Data: 600 time points, 200 from each of x, y, and z acceleration</li>
<li>Accuracy on model trained and tested on all 14 gestures was very poor (10-20% accuracy).</li>
<li>Did gesture set robustness determination by training and testing a different model on each of the 14 choose 4 = 1001 combinations of 4 gestures (so trained on 24 gestures, tested on 12)
<ul>
<li>11 models got an accuracy of 83%</li>
<li>Of these 11 models, counted the occurrences of specific gestures in their gesture sets</li>
</ul>
</li>
</ul>
<figure><img src="img/session_3_good_gestures.png"
         alt="Table of combinations of gestures with accuracy higher than 80%* a gesture set comprising of 2 (swipe right), 4 (tap), 6 (counter clock-wise circle), and 9 (poke) was determined to be the most robustly differentiable."/><figcaption>
            <p>Table of combinations of gestures with accuracy higher than 80%* a gesture set comprising of 2 (swipe right), 4 (tap), 6 (counter clock-wise circle), and 9 (poke) was determined to be the most robustly differentiable.</p>
        </figcaption>
</figure>

<figure><img src="img/session_3_good_gestures_histogram.png"
         alt="Histogram of gesture number (x-axis) vs. occurrence count in the 11 good models (y-axis)"/><figcaption>
            <p>Histogram of gesture number (x-axis) vs. occurrence count in the 11 good models (y-axis)</p>
        </figcaption>
</figure>

<p>The most robustly differentiable gesture set is:</p>
<ul>
<li>2 (swipe right)</li>
<li>4 (downward swipe) &ndash; aka &ldquo;downward tap&rdquo;, as if typing on a laptop keyboard</li>
<li>6 (counter-clockwise circle starting from the bottom)</li>
<li>9 (forward tap) &ndash; aka &ldquo;poke&rdquo;, as if pressing a elevator button</li>
</ul>
<p>Swipe right, downward swipe, and forward tap all have acceleration and return movement most strongly around a distinct axis (x, y, and z-axis). Counter-clockwise circle starting from the bottom has a distinct pattern of multiple acceleration directions and those directions are opposite to the movements in the other gestures (i.e. it starts with an upward movement whereas downward swipe is going down at that time point). Thus, even with common-sense reasoning this gesture set seems to make sense.</p>
<h3 id="session-4-april-2-successful-data-collection----1st-big-data-set----1-subject-gati">Session 4 (April 2) Successful Data Collection &ndash; 1st Big Data Set &ndash; 1 Subject (Gati)</h3>
<ul>
<li><em>Gesture Collection App - v1</em></li>
<li><em>Gesture Set - v1:</em> 4 gestures, give subject 3 seconds to perform a gesture</li>
<li><em>Trials:</em> 15 trials of 4 gestures each repeated 10 times (40 gestures per trial), collected on 1 subject (Gati) at 6 different hand orientations (90 deg &ndash; face level to -90 deg &ndash; below chair) to create extra noise</li>
<li>Also attempted 4 times to collect data while walking, but app kept stopping mid-way through trial + hard to see gesture prompts when walking around</li>
</ul>
<h4 id="findings--process-iterations-2">Findings / Process Iterations:</h4>
<ul>
<li>jupyter notebook <code>session_4/explore_data_session_4.ipynb</code> has code for loading, processing, and analyzing the data</li>
<li>taps were messy (expected 80 start and end taps, got 78, 79, etc.)
<ul>
<li>Decided to write code to detect start of gestures
<ul>
<li>Find when acceleration in x, y, or z crosses threshold of <code>0.4</code>. This is a peak indicating that an intentional movement has been started.</li>
<li>Extract window <code>25 time points before peak</code>. This is the detected start.</li>
<li>Extract window <code>75 time points after peak</code>. This is the detected end.</li>
<li>Have windows of 100 time points (shorter window to get even closer to the active gesture period)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure><img src="img/session_4_traces_g2.png"
         alt="Plot of all gesture 2 (swipe right) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)"/><figcaption>
            <p>Plot of all gesture 2 (swipe right) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)</p>
        </figcaption>
</figure>

<figure><img src="img/session_4_traces_g4.png"
         alt="Plot of all gesture 4 (swipe left) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)"/><figcaption>
            <p>Plot of all gesture 4 (swipe left) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)</p>
        </figcaption>
</figure>

<figure><img src="img/session_4_traces_g6.png"
         alt="Plot of all gesture 6 (counter-clockwise circle starting from the bottom) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)"/><figcaption>
            <p>Plot of all gesture 6 (counter-clockwise circle starting from the bottom) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)</p>
        </figcaption>
</figure>

<figure><img src="img/session_4_traces_g9.png"
         alt="Plot of all gesture 9 (tap forward) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)"/><figcaption>
            <p>Plot of all gesture 9 (tap forward) instances in train-set (x-acceleration only, trace of window 200 ms after starting tap)</p>
        </figcaption>
</figure>

<h4 id="model-performance">Model Performance</h4>
<ul>
<li>Train data: 12 of 15 trials</li>
<li>Test data: 3 of 15 trials (cross validation, hold out all combinations of 3 test trials. Note that test and train data are from different trials, so the test gestures were performed at a different hand orientation from the train gestures. This is a way of checking for gesture robustness)</li>
<li>Data: 300 time points, 100 from each of x, y, and z acceleration</li>
<li>Accuracy was good enough for a baseline model</li>
</ul>
<figure><img src="img/session_4_cross_validation_accuracy.png"
         alt="session 4 cross validation accuracy"/><figcaption>
            <p>session 4 cross validation accuracy</p>
        </figcaption>
</figure>

<p>Model tends to have accuracy greater than 50% (4 gestures random chance guessing is 25%). Note that this test of accuracy is harder than normal trials would be because each trial is at a different hand orientation (some of which are very extreme and unrealistic). So the model learning this pattern is a good sign of gesture robustness.</p>
<h3 id="session-5-april-5-successful-data-collection----2nd-big-data-set----16-subjects-step-students">Session 5 (April 5) Successful Data Collection &ndash; 2nd Big Data Set &ndash; 16 subjects (STEP students)</h3>
<ul>
<li><em>Gesture Collection App - v2:</em> random order gesture prompts incorporated into the app. Start and end of window recorded by app (no need for start and end mark taps). Ability to name trials in the app (aid mapping to metadata file)
<ul>
<li>App prompts users to
<ul>
<li>STUDY prompt (2 seconds)</li>
<li>MAKE gesture (3 seconds)</li>
<li>STOP gesture (2 seconds)</li>
</ul>
</li>
</ul>
</li>
<li><em>Gesture Set - v2:</em> 4 gestures, give subject 3 seconds to perform a gesture</li>
<li><em>Trials:</em> 55 total trials of 4 gestures each repeated 3 times (12 gestures per trial), collected on 26 subjects (STEP students) at 3 different realistic trial conditions
<ul>
<li>A. comfortable hand is supported (least noisy)</li>
<li>B. comfortable but hand is not supported (little noisy)</li>
<li>C. discrete / hidden gesture (most noisy, lets see what people do)</li>
</ul>
</li>
</ul>
<h4 id="findings--process-iterations-3">Findings / Process Iterations:</h4>
<ul>
<li>jupyter notebook <code>session_5/explore_data_session_5.ipynb</code> has code for loading, processing, and analyzing the data</li>
<li>Lots of people did a false start during the STUDY prompt, had to manually inspect and remove 20 trials because the peak-based windowing code does not work if there are 2 gestures within a gesture window period (kept 35 trials)</li>
<li>People have very different ideas of how to be sneaky. Tends to be either small motion in plain sight, or regular motion but with hand hidden in lap or under table.</li>
<li>PEOPLE DO NOT LIKE FORWARD TAP! They do not want to move on the z-axis (forward) when all other gestures are in the same x-y plane
<ul>
<li>People more open to the idea of a &ldquo;flick&rdquo; motion</li>
</ul>
</li>
</ul>
<figure><img src="img/session_5_good_trial.png"
         alt="Example of good trial: no false starts. Orange dots mark the start timestamp of the MAKE prompt, purple dots mark the detected start (-25 from the first threshold crossing) timestamp. Starts plotted separately for clarity and ease of manual review."/><figcaption>
            <p>Example of good trial: no false starts. Orange dots mark the start timestamp of the MAKE prompt, purple dots mark the detected start (-25 from the first threshold crossing) timestamp. Starts plotted separately for clarity and ease of manual review.</p>
        </figcaption>
</figure>

<figure><img src="img/session_5_bad_trial.png"
         alt="Example of trial with false starts: many false starts. False starts ruin the gesture windowing process."/><figcaption>
            <p>Example of trial with false starts: many false starts. False starts ruin the gesture windowing process.</p>
        </figcaption>
</figure>

<p>Model tended to have decent accuracy (around 70%+) on the cross validation accuracy tests (same manner as session 4)</p>
<figure><img src="img/session_4_cross_validation_accuracy.png"
         alt="session 4 cross validation accuracy"/><figcaption>
            <p>session 4 cross validation accuracy</p>
        </figcaption>
</figure>

<p>The reasonably high model accuracy is a very good sign because in some of the cross validation instances, the test set gestures came from different people/trial conditions than the train set gestures and the model still generalized well.</p>
<h3 id="demo-day-1-april-7">Demo Day 1 (April 7)</h3>
<ul>
<li>The demo day model consisted of gestures from session 4 and session 5</li>
<li>The app detected the starts and windowed the (-25 before the peak, +75 after the peak) IMU data</li>
<li>A flask web service running the model responded to HTTP requests made from the app with model predictions
<ul>
<li>The &ldquo;flick&rdquo; motion is frequently classified as &ldquo;forward tap&rdquo; so they are interchangeable. Henceforth, &ldquo;flick&rdquo; replaced forward tap.</li>
</ul>
</li>
</ul>
<h3 id="unfortunate-timing-april-10---april-23">Unfortunate Timing (April 10 - April 23)</h3>
<ul>
<li>Gati had 6 hour play rehearsals or performances <em>every day</em> for 2 weeks and did not have time to collect data</li>
</ul>
<h3 id="session-6-april-25-trouble-shooting-process">Session 6 (April 25) Trouble-Shooting Process</h3>
<ul>
<li><em>Gesture Collection App - v3:</em>
<ul>
<li>Redesign of App to lessen false starts by having prompts only for
<ul>
<li>MAKE gesture (3 seconds)</li>
<li>STOP gesture (2 seconds) + black out to indicate that user should stop making the gesture</li>
</ul>
</li>
<li>Clean up visual layout of the app</li>
<li>Try to make trial shorter whenever possible &ndash; reduce amount of time to name trial in order to start trial quicker</li>
<li>Chime sound when data is successfully uploaded to firebase</li>
</ul>
</li>
</ul>
<h4 id="findings--process-iterations-4">Findings / Process Iterations:</h4>
<ul>
<li>When cleaning-up visual layout of the app, the print outs of IMU accelerations were removed. Unfortunately, while ugly, those very necessary &ndash; they give the data collecting person an idea of whether the App and IMU are connected. Very anxiety inducing to collect data without knowing if it was being recorded. Asked for the print-outs to come back.</li>
<li>Not enough time to enter trial name (was asking for people to give Olin username as a unique ID). Could not actually collect trials reasonably.</li>
<li>Did have conversations/surveyed people about activation gestures. People tended to like the twist gesture for activation
<ul>
<li>Twist gesture is easily detectable (180 flip in hand/ring orientation and then 180 back)</li>
<li>But was eliminated initially from the machine learning gesture recognition because a twist is performed differently on right vs. left hand and controlling for that would be a hassle (training separate models for rings on left vs. right hands, an extra configuration setting)</li>
<li>But great as a wake up gesture! Making a note here, but wake-up gesture would be a software demo app implementation task, not a signal-processing machine learning task,</li>
</ul>
</li>
</ul>
<h3 id="session-7-april-27-28-successful-data-collection----3rd-big-data-set----16-subjects-olin--prospective-students">Session 7 (April 27-28) Successful Data Collection &ndash; 3rd Big Data Set &ndash; 16 subjects (Olin / Prospective students)</h3>
<ul>
<li><em>Gesture Collection App - v4:</em> fixed major issues in app, now decently workable</li>
<li><em>Gesture Set - v3:</em> 4 gestures (&ldquo;flick&rdquo; replaced &ldquo;forward tap&rdquo;), give subject 3 seconds to perform a gesture</li>
<li><em>Trials:</em> 36 total trials of 4 gestures each repeated 3 times (12 gestures per trial), collected on 13 subjects (mainly Olin and prospective students at SLAC event) at 2 different realistic trial conditions
<ul>
<li>A. comfortable</li>
<li>B. discrete gesture: strange orientation, either with arm at side or arms crossed</li>
</ul>
</li>
</ul>
<h4 id="findings--process-iterations-5">Findings / Process Iterations:</h4>
<ul>
<li>Documented proper way to do gestures in a <a href="https://docs.google.com/presentation/d/16hITbMB7vJ_tKl1JPVFF_6W_iE291YWYdGt78I2dObw/edit?usp=sharing">slideshow</a>. People don&rsquo;t like reading text, but the pictures were helpful to point at when explaining gestures to people</li>
<li>Most people are fine with twist as a wake-up gesture</li>
</ul>
<h3 id="demo-day-2-may-2-successful-ml-model-integration">Demo Day 2 (May 2) Successful ML-Model Integration</h3>
<ul>
<li>The demo day model consisted of gestures from session 4, session 5, and session 7
<ul>
<li>final model: <code>demo_day/pca_svm_full_model_session_4_and_5_and_7.pkl</code>, accuracy roughly 75%</li>
<li>most common error mode: &ldquo;flick&rdquo; vs. &ldquo;downward tap&rdquo;</li>
</ul>
</li>
<li>The app detected the starts and windowed the (-25 before the peak, +75 after the peak) IMU data</li>
<li>A flask web service running the model responded to HTTP requests made from the app with model predictions</li>
</ul>
<h2 id="next-steps">Next Steps</h2>
<h3 id="model">Model</h3>
<p>Failure Mode 1: confusion between &ldquo;flick&rdquo; vs. &ldquo;downward tap&rdquo;</p>
<ul>
<li>Potential Solution: incorporate orientation information from the IMU gyroscope sensor</li>
<li>Can probably express quaternians in terms of relative change in respect to the first quaternian of the detected gesture
<ul>
<li>Orientation respective to gravity DOES NOT MATTER &ndash; people want to do gestures at strange orientations (at side, in pockets, in bed)</li>
</ul>
</li>
<li>Should talk to Paul Ruvolo about future work on this</li>
</ul>
<p>Failure Mode 2: inherent time problems with time-based signal (i.e. if you do the gesture too slowly, if you speed up and slow down at different points than people in the train set)</p>
<ul>
<li>Potential Solution: CNN + DTW
<ul>
<li>Convolutional Neural Network (CNN): pools values so that is signal is shifted it still registers as roughly the same signal</li>
<li>Dynamic Time Warping (DTW): learns pattern of matching time signals to a template, warps signal to match template</li>
</ul>
</li>
<li>Both of these are deep learning techniques that require far more data to learn effectively</li>
</ul>
<h3 id="data-collection">Data Collection</h3>
<ul>
<li>The data collection pipeline is now solid!</li>
<li>Maybe more work can be done for effectively isolating and discarding individual gestures (instead of tossing the entire trial)</li>
<li>The bottle neck in data collection is teaching people the gestures (always takes ~3 minutes) and then people only have about 2 minutes of patience left to actually collect data</li>
<li>Collecting data in the last month of school was a mistake (too many things happening, people are too busy, nobody has time to really give you more than 2 minutes of time)</li>
</ul>
<h3 id="software-integration">Software Integration</h3>
<ul>
<li>Twist as a wake up gesture</li>
<li>Running model inside Swift iOS app (compared to external Flask Service)</li>
</ul></article>
      </div>
    </section>
  </div>

  
  <footer id="footer" class="wrapper style1-alt">
  <div class="inner">
    <ul class="menu">
      <li>&copy; 2022 <a href="/">Gati Aher</a></li>
      <li>Powered by <a href="https://gohugo.io/">Hugo</a></li>
      <li>
        Adapted 
        <a href="https://html5up.net/hyperspace"
          > from HTML5 UP</a
        >
      </li>
      <li>
        Code on <a href="https://github.com/GatiAher/gatiaher-hugo">GitHub</a>
      </li>
    </ul>
  </div>
</footer>

</div>



    
    

<script src="/js/jquery.min.js"></script>

<script src="/js/jquery.scrollex.min.js"></script>

<script src="/js/jquery.scrolly.min.js"></script>

<script src="/js/browser.min.js"></script>

<script src="/js/breakpoints.min.js"></script>

<script src="/js/util.js"></script>

<script src="/js/main.js"></script>


<script src="/js/tab_control.js"></script>


<script src="/js/carousel_control.js"></script>

  </body>
</html>
