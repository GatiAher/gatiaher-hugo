<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis on Gati Aher</title>
    <link>http://GatiAher.github.io/categories/data-analysis/</link>
    <description>Recent content in Data Analysis on Gati Aher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 May 2022 14:53:05 -0400</lastBuildDate><atom:link href="http://GatiAher.github.io/categories/data-analysis/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>IMU Gesture Recognition</title>
      <link>http://GatiAher.github.io/projects/imu-gesture-recognition/</link>
      <pubDate>Sun, 15 May 2022 14:53:05 -0400</pubDate>
      
      <guid>http://GatiAher.github.io/projects/imu-gesture-recognition/</guid>
      <description>&lt;p&gt;This project analyzed and prepared a baseline machine learning model to perform gesture recognition on data collected with &lt;a href=&#34;https://mbientlab.com/store/metamotionrl/&#34;&gt;MbientLab MMRL IMU&lt;/a&gt; rubber-banded to a two_finger ring. The final baseline model was trained with data from sessions 4, 5, and 7 consisting of 1,212 total instances of 4 gestures collected across 27 people.&lt;/p&gt;
&lt;p&gt;When trained with a train-test split of 80:20, the model had an accuracy of 75%. The final model trained with full data (no train-test split) had reasonably robust performance in the real-time system (successfully generalized its gestures predictions to other people when integrated with the software demo app).&lt;/p&gt;
&lt;p&gt;This report provides details on deciding on a gesture set, building and refining the gesture data collection process, and steps to integrate the model with the software iOS demo app.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
  &lt;ul class=&#34;actions&#34;&gt;
       
    &lt;li&gt;
      &lt;a href=&#34;https://github.com/OlinSTEP/signal-processing-gesture-data-collection&#34; class=&#34;button icon outline brands fa-github&#34;&gt;Visit GitHub&lt;/a&gt;
    &lt;/li&gt;
    
  &lt;/ul&gt;
&lt;/div&gt;



&lt;div id=&#34;Container&#34;
 style=&#34;padding-bottom:56.25%; position:relative; display:block; width: 100%&#34;&gt;
 &lt;iframe id=&#34;googleSlideIframe&#34;
  width=&#34;100%&#34; height=&#34;100%&#34;
  src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vT1MJqD9l2cEu7DgdtxDkJb_aH5ysP1NklfoNUuvb3sVjp9z3MezS0HFBOv-fLhV7ESwZks_xU1Z-wQ/embed?start=false&amp;amp;loop=true&amp;amp;delayms=3000&#34;
  frameborder=&#34;0&#34; allowfullscreen=&#34;&#34;
  style=&#34;position:absolute; top:0; left: 0&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Handwriting Detection With Faster R-CNN &#43; Experiments</title>
      <link>http://GatiAher.github.io/projects/handwriting-detection-with-faster-r-cnn-plus-experiments/</link>
      <pubDate>Fri, 20 Aug 2021 10:43:34 -0400</pubDate>
      
      <guid>http://GatiAher.github.io/projects/handwriting-detection-with-faster-r-cnn-plus-experiments/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://indico.io/&#34;&gt;Indico Data Solutions&lt;/a&gt; provides services to extract information from scanned pdfs. Since their existing OCR + NLP pipeline did not extract handwriting, one of my internship projects involved creating a robust solution to detect and classify handwriting using a deep learning computer vision model.&lt;/p&gt;
&lt;p&gt;I started by fine-tuning upon the &lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;Faster R-CNN model&lt;/a&gt; from the &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;Detectron-v2 framework&lt;/a&gt;. Then I tried to improve upon the baseline performance with (i) different pre-training tasks, (ii) multi-label formulation, (iii) strategies to improve small object detection, and (iv) different label sets and datasets. This report documents my methods and finishes with a class confusion analysis and retrospective.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EMG Gesture Recognition</title>
      <link>http://GatiAher.github.io/projects/emg-gesture-recognition.md/</link>
      <pubDate>Mon, 10 May 2021 21:39:39 -0500</pubDate>
      
      <guid>http://GatiAher.github.io/projects/emg-gesture-recognition.md/</guid>
      <description>&lt;p&gt;This project followed the topological data analysis steps laid out in &lt;a href=&#34;http://doi.org/10.1098/rsif.2017.0734&#34;&gt;Phinyomark et al (2017) &amp;ldquo;Navigating features: a topologically informed chart of electromyographic features space&amp;rdquo;&lt;/a&gt; to analyze 43 features extracted from surface EMG signals of three gestures (rock, paper, scissors) performed by a single subject.&lt;/p&gt;
&lt;p&gt;I used sklearn&amp;rsquo;s topological data analysis tools with the Kepler Mapper and Ward&amp;rsquo;s minimum variance method as the criterion for hierarchial clustering in order to analyze feature redundancy. Comparison of feature class separability was analyzed by calculating Davies-Bouldin index (DBI) and Fisher&amp;rsquo;s linear discriminant index (FLDI), and measuring misclassification rates. 5-fold cross validation Linear Discriminant Analysis and Support Vector Machine were employed as classifiers.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
  &lt;ul class=&#34;actions&#34;&gt;
       
    &lt;li&gt;
      &lt;a href=&#34;https://github.com/GatiAher/EMG_Gesture_Recognition&#34; class=&#34;button icon outline brands fa-github&#34;&gt;Visit GitHub&lt;/a&gt;
    &lt;/li&gt;
    
  &lt;/ul&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Facial Recognition Using Principal Component Analysis</title>
      <link>http://GatiAher.github.io/projects/facial-recognition-using-principal-component-analysis/</link>
      <pubDate>Wed, 14 Apr 2021 13:12:04 -0400</pubDate>
      
      <guid>http://GatiAher.github.io/projects/facial-recognition-using-principal-component-analysis/</guid>
      <description>&lt;p&gt;I have used multiple variations of Principal Component Analysis (PCA) in my research on microbial community analysis. To explain the core theory and assumptions of PCA to my lab group, I fleshed out an analysis of a toy example (eigenfaces) that I had originally seen in class. This analysis reasons about the assumptions of PCA and the effects of applying it to out-of-distribution data by using PCA to perform facial recognition on a dataset of my classmates&amp;rsquo; faces.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I was invited to present this project at &lt;a href=&#34;https://ms-my.facebook.com/MathUpConference/photos/because-the-mathup-conference-is-conducted-in-both-polish-and-english-another-du/2805409529774282&#34;&gt;Łódź University&amp;rsquo;s SP2021 [virtual] MathUp conference&lt;/a&gt; at Łódź University, Poland. The conference was an incredibly fun opportunity to meet fellow researchers and data scientists!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Fourier Transform Detective Story!</title>
      <link>http://GatiAher.github.io/projects/a-fourier-transform-detective-story/</link>
      <pubDate>Sun, 14 Mar 2021 22:24:10 -0400</pubDate>
      
      <guid>http://GatiAher.github.io/projects/a-fourier-transform-detective-story/</guid>
      <description>&lt;p&gt;A group of research students had tried to use a 2D discrete Fourier Transform to characterize the pattern of repeating protein units on a bacteria surface layer image. Since their spectral graph looked unusually messy and their pattern estimates seemed very wrong, my research professor asked me to take a crack at interpreting and cleaning it. This was a neat application of understanding the mathematical assumptions of a technique to properly isolate and interpret results.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
